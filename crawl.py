import pandas as pd
import requests
from bs4 import BeautifulSoup
from nba_api.stats.endpoints import leaguegamefinder
import time
from datetime import datetime, timedelta

############################################
# 1ï¸âƒ£ NBA_API ê¸°ë°˜ ìˆ˜ì§‘ (1ì°¨ ì‹œë„)
############################################

def fetch_with_nba_api():
    print("ğŸ“¡ [PRIMARY] stats.nba.com (nba_api) ì‹œë„ ì¤‘...")

    seasons = ['2025-26', '2024-25', '2023-24']
    all_dfs = []

    for season in seasons:
        for attempt in range(1, 4):
            try:
                print(f"  â”” ì‹œì¦Œ {season} ì‹œë„ {attempt}/3")
                gf = leaguegamefinder.LeagueGameFinder(
                    league_id_nullable='00',
                    season_nullable=season
                )
                df = gf.get_data_frames()[0]

                if not df.empty:
                    all_dfs.append(df)
                    print(f"    âœ… ì‹œì¦Œ {season} ì„±ê³µ ({len(df)}ê²½ê¸°)")
                    break

            except Exception as e:
                print(f"    âš ï¸ ì‹¤íŒ¨: {e}")
                time.sleep(10)

        time.sleep(8)

    if not all_dfs:
        raise Exception("nba_api ì „ì²´ ì‹¤íŒ¨")

    final_df = pd.concat(all_dfs, ignore_index=True)
    final_df['GAME_DATE'] = pd.to_datetime(final_df['GAME_DATE'])
    final_df = final_df[final_df['GAME_DATE'] >= pd.Timestamp('2024-01-01')]
    final_df = final_df.sort_values('GAME_DATE', ascending=False)

    return final_df


############################################
# 2ï¸âƒ£ NBA ê³µì‹ CDN ê¸°ë°˜ ìˆ˜ì§‘ (Fallback)
############################################

def fetch_with_nba_cdn(days=120):
    print("ğŸŒ [FALLBACK] NBA ê³µì‹ CDN(JSON) ì‚¬ìš©")

    end_date = datetime.utcnow().date()
    start_date = end_date - timedelta(days=days)

    games = []

    date = start_date
    while date <= end_date:
        date_str = date.strftime("%Y-%m-%d")
        url = f"https://cdn.nba.com/static/json/liveData/scoreboard/todaysScoreboard_{date_str}.json"

        try:
            res = requests.get(url, timeout=10)
            if res.status_code == 200:
                data = res.json()
                game_list = data.get("scoreboard", {}).get("games", [])

                for g in game_list:
                    games.append({
                        "GAME_ID": g["gameId"],
                        "GAME_DATE": date_str,
                        "TEAM_ID": g["homeTeam"]["teamId"],
                        "TEAM_ABBREVIATION": g["homeTeam"]["teamTricode"],
                        "MATCHUP": f"{g['awayTeam']['teamTricode']} @ {g['homeTeam']['teamTricode']}",
                        "PTS": g["homeTeam"].get("score", 0),
                        "WL": None
                    })
                    games.append({
                        "GAME_ID": g["gameId"],
                        "GAME_DATE": date_str,
                        "TEAM_ID": g["awayTeam"]["teamId"],
                        "TEAM_ABBREVIATION": g["awayTeam"]["teamTricode"],
                        "MATCHUP": f"{g['awayTeam']['teamTricode']} @ {g['homeTeam']['teamTricode']}",
                        "PTS": g["awayTeam"].get("score", 0),
                        "WL": None
                    })

        except Exception:
            pass

        date += timedelta(days=1)
        time.sleep(0.3)

    if not games:
        raise Exception("CDN ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

    df = pd.DataFrame(games)
    df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])
    df = df.sort_values('GAME_DATE', ascending=False)

    print(f"âœ… CDN ê¸°ë°˜ ê²½ê¸° {len(df)}ê±´ ìˆ˜ì§‘")
    return df


############################################
# 3ï¸âƒ£ CBS Sports ë¶€ìƒ/ë‰´ìŠ¤ (ê¸°ì¡´ ìœ ì§€)
############################################

def fetch_cbs_news():
    print("ğŸ“° CBS Sports ë¶€ìƒ/ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

    url = "https://www.cbssports.com/nba/injuries/"
    headers = {'User-Agent': 'Mozilla/5.0'}
    res = requests.get(url, headers=headers, timeout=20)
    soup = BeautifulSoup(res.text, 'html.parser')

    news_data = []

    team_sections = soup.find_all('div', class_='TableBase')

    for section in team_sections:
        team = section.find('span', class_='TeamName')
        if not team:
            continue

        team_name = team.text.strip()
        rows = section.find_all('tr', class_='TableBase-bodyTr')

        issues = []
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 5:
                issues.append(
                    f"{cols[4].text.strip()}: {cols[0].text.strip()} ({cols[3].text.strip()})"
                )

        if issues:
            news_data.append({
                "TEAM": team_name,
                "NEWS": " | ".join(issues)
            })

    if news_data:
        pd.DataFrame(news_data).to_csv("nba_news.csv", index=False, encoding="utf-8-sig")
        print("âœ… ë¶€ìƒ/ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")


############################################
# 4ï¸âƒ£ ë©”ì¸ ì‹¤í–‰ ë¡œì§
############################################

def collect_real_data():
    print("ğŸš€ NBA ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

    try:
        df = fetch_with_nba_api()
        source = "nba_api"
    except Exception as e:
        print(f"âŒ nba_api ì‹¤íŒ¨ â†’ CDN fallback ì „í™˜ ({e})")
        df = fetch_with_nba_cdn()
        source = "nba_cdn"

    df.to_csv("nba_history_3years.csv", index=False, encoding="utf-8-sig")
    print(f"ğŸ“ ê²½ê¸° ë°ì´í„° ì €ì¥ ì™„ë£Œ ({source})")

    fetch_cbs_news()
    print("ğŸ crawl.py ì¢…ë£Œ")


if __name__ == "__main__":
    collect_real_data()
