import pandas as pd
import requests
from bs4 import BeautifulSoup
from nba_api.stats.endpoints import leaguegamefinder
import time
from datetime import datetime
import os

def collect_real_data():
    print("ğŸš€ [1/2] NBA ìµœì‹ (2025-26 ì‹œì¦Œ í¬í•¨) ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

    try:
        all_dfs = []

        # ì‹œì¦Œë³„ ëª…ì‹œì  í˜¸ì¶œ (ì•ˆì •ì„± í•µì‹¬)
        target_seasons = ['2025-26', '2024-25', '2023-24']
        season_id_map = {
            '2025-26': '22025',
            '2024-25': '22024',
            '2023-24': '22023'
        }

        for season in target_seasons:
            print(f"ğŸ“¡ ì‹œì¦Œ {season} ìˆ˜ì§‘ ì¤‘...")
            game_finder = leaguegamefinder.LeagueGameFinder(
                league_id_nullable='00',
                season_nullable=season
            )
            df = game_finder.get_data_frames()[0]
            all_dfs.append(df)
            time.sleep(1.5)  # rate limit ë°©ì§€

        all_games = pd.concat(all_dfs, ignore_index=True)

        # ì‹œì¦Œ í•„í„° (ì´ì¤‘ ì•ˆì „ì¥ì¹˜)
        final_df = all_games[
            all_games['SEASON_ID'].isin(season_id_map.values())
        ].copy()

        final_df['GAME_DATE'] = pd.to_datetime(final_df['GAME_DATE'])
        final_df = final_df.sort_values('GAME_DATE', ascending=False)

        final_df.to_csv('nba_history_3years.csv', index=False, encoding='utf-8-sig')

        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(final_df)}ê±´ ì €ì¥ (nba_history_3years.csv)")
        print(f"ğŸ“… ë°ì´í„° ë²”ìœ„: {final_df['GAME_DATE'].min().date()} ~ {final_df['GAME_DATE'].max().date()}")

    except Exception as e:
        print(f"âŒ ê¸°ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    print("\nğŸš€ [2/2] ì‹¤ì‹œê°„ ë¶€ìƒì ë° ì£¼ìš” íŒ€ ë‰´ìŠ¤(CBS Sports) í†µí•© í¬ë¡¤ë§ ì¤‘...")

    try:
        url = "https://www.cbssports.com/nba/injuries/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        res = requests.get(url, headers=headers, timeout=20)
        soup = BeautifulSoup(res.text, 'html.parser')

        news_data = []

        team_sections = soup.find_all('div', class_='TableBase')

        for section in team_sections:
            try:
                team_name = section.find('span', class_='TeamName')
                if not team_name:
                    continue

                team_name_raw = team_name.text.strip()
                rows = section.find_all('tr', class_='TableBase-bodyTr')

                team_issues = []

                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) >= 5:
                        player = cols[0].text.strip()
                        injury = cols[3].text.strip()
                        status = cols[4].text.strip()
                        team_issues.append(f"{status}: {player}({injury})")

                if team_issues:
                    news_data.append({
                        'TEAM': team_name_raw,
                        'NEWS': " | ".join(team_issues)
                    })

            except Exception:
                continue

        if news_data:
            summary = pd.DataFrame(news_data)
            summary.to_csv('nba_news.csv', index=False, encoding='utf-8-sig')
            print(f"âœ… í†µí•© ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(summary)}ê°œ íŒ€)")
        else:
            print("âš ï¸ í˜„ì¬ ì—…ë°ì´íŠ¸ëœ ì£¼ìš” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    collect_real_data()
