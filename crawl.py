import pandas as pd
import requests
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from nba_api.stats.endpoints import leaguegamefinder


# =========================
# 1ï¸âƒ£ PRIMARY: nba_api (ì‹œë„ë§Œ í•˜ê³  ì‹¤íŒ¨í•´ë„ OK)
# =========================
def fetch_with_nba_api():
    print("ğŸ“¡ [PRIMARY] stats.nba.com (nba_api) ì‹œë„ ì¤‘...")

    seasons = ["2025-26", "2024-25", "2023-24"]
    dfs = []

    for season in seasons:
        for attempt in range(3):
            try:
                print(f"  â”” ì‹œì¦Œ {season} ì‹œë„ {attempt+1}/3")
                gf = leaguegamefinder.LeagueGameFinder(
                    league_id_nullable="00",
                    season_nullable=season
                )
                df = gf.get_data_frames()[0]
                if not df.empty:
                    dfs.append(df)
                break
            except Exception as e:
                print(f"    âš ï¸ ì‹¤íŒ¨: {e}")
                time.sleep(5)

    if not dfs:
        raise Exception("nba_api ì „ì²´ ì‹¤íŒ¨")

    final_df = pd.concat(dfs, ignore_index=True)
    final_df["GAME_DATE"] = pd.to_datetime(final_df["GAME_DATE"])
    return final_df


# =========================
# 2ï¸âƒ£ FALLBACK: NBA ê³µì‹ CDN (í•µì‹¬ í•´ê²°ì±…)
# =========================
def fetch_with_nba_cdn(days=450):
    print("ğŸŒ [FALLBACK] NBA ê³µì‹ CDN(JSON) ì‚¬ìš©")

    end_date = datetime.utcnow().date()
    start_date = end_date - timedelta(days=days)

    games = []

    date = start_date
    while date <= end_date:
        date_str = date.strftime("%Y%m%d")  # ğŸ”¥ ì¤‘ìš”
        url = f"https://cdn.nba.com/static/json/liveData/scoreboard/scoreboard_{date_str}.json"

        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                date += timedelta(days=1)
                continue

            data = res.json()
            game_list = data.get("scoreboard", {}).get("games", [])

            for g in game_list:
                matchup = f"{g['awayTeam']['teamTricode']} @ {g['homeTeam']['teamTricode']}"

                games.append({
                    "GAME_ID": g["gameId"],
                    "GAME_DATE": date,
                    "TEAM_ID": g["homeTeam"]["teamId"],
                    "TEAM_ABBREVIATION": g["homeTeam"]["teamTricode"],
                    "MATCHUP": matchup,
                    "PTS": g["homeTeam"].get("score", 0),
                    "WL": None
                })

                games.append({
                    "GAME_ID": g["gameId"],
                    "GAME_DATE": date,
                    "TEAM_ID": g["awayTeam"]["teamId"],
                    "TEAM_ABBREVIATION": g["awayTeam"]["teamTricode"],
                    "MATCHUP": matchup,
                    "PTS": g["awayTeam"].get("score", 0),
                    "WL": None
                })

        except Exception:
            pass

        date += timedelta(days=1)
        time.sleep(0.25)

    if not games:
        raise Exception("CDN ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

    df = pd.DataFrame(games)
    df["GAME_DATE"] = pd.to_datetime(df["GAME_DATE"])
    df = df.sort_values("GAME_DATE", ascending=False)

    print(f"âœ… CDN ê¸°ë°˜ ê²½ê¸° {len(df)}ê±´ ìˆ˜ì§‘ ì„±ê³µ")
    return df


# =========================
# 3ï¸âƒ£ CBS Sports ë¶€ìƒì ë‰´ìŠ¤
# =========================
def fetch_injury_news():
    print("ğŸš‘ [NEWS] CBS Sports ë¶€ìƒì ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

    url = "https://www.cbssports.com/nba/injuries/"
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    news_data = []

    tables = soup.find_all("table")
    for table in tables:
        rows = table.find_all("tr")
        for row in rows[1:]:
            cols = row.find_all("td")
            if len(cols) >= 5:
                player = cols[0].text.strip()
                team = cols[1].text.strip()
                injury = cols[3].text.strip()
                status = cols[4].text.strip()

                news_data.append({
                    "TEAM": team,
                    "NEWS": f"{status}: {player} ({injury})"
                })

    if news_data:
        df = pd.DataFrame(news_data)
        df.to_csv("nba_news.csv", index=False, encoding="utf-8-sig")
        print(f"âœ… ë¶€ìƒì ë‰´ìŠ¤ {len(df)}ê±´ ì €ì¥")
    else:
        print("âš ï¸ ë¶€ìƒì ë‰´ìŠ¤ ì—†ìŒ")


# =========================
# 4ï¸âƒ£ ë©”ì¸ íŒŒì´í”„ë¼ì¸
# =========================
def collect_real_data():
    print("ğŸš€ NBA ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

    try:
        df = fetch_with_nba_api()
        print("âœ… nba_api ì„±ê³µ")
    except Exception as e:
        print(f"âŒ nba_api ì‹¤íŒ¨ â†’ CDN fallback ì „í™˜ ({e})")
        df = fetch_with_nba_cdn()

    df.to_csv("nba_history_3years.csv", index=False, encoding="utf-8-sig")
    print(f"ğŸ“ nba_history_3years.csv ì €ì¥ ì™„ë£Œ ({len(df)}ê±´)")

    fetch_injury_news()


if __name__ == "__main__":
    collect_real_data()

